{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining - Handin 2 - Graph mining\n",
    "\n",
    "Welcome to the handin on clustering algorithms and outlier detection. \n",
    "This handin corresponds to the topics in Week 10--15 in the course.\n",
    "\n",
    "The handin IS \n",
    "* mandatory\n",
    "* done individually\n",
    "* worth 10% of the grade\n",
    "\n",
    "For the handin, you will prepare a report in PDF format, by exporting the Jupyter notebook. \n",
    "Please submit\n",
    "1. The jupyter notebook file with your answers\n",
    "2. The PDF obtained by exporting the jupyter notebook\n",
    "\n",
    "Submit both files on Blackboard no later than **April 20th kl. 13.59**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT TOUCH THESE IMPORTS\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities.load_data import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical questions\n",
    "\n",
    "We will start out with theory questions. These questions are theoretical; as such, they **can and should be answered without implementation**. In particular, you are allowed to verify your answer experimentally, but the motivation you provide should be only in text and theoretically grounded. \n",
    "\n",
    "### Spectral theory and graph partitioning\n",
    "\n",
    "Let $L = \\Delta - A$ the Laplacian matrix of a graph $G$ and be $\\lambda_1 \\geq \\lambda_2 ... \\geq \\lambda_n$ be the eigenvalues of $L$ with eigenvectors $\\textbf{u}_1, ..., \\textbf{u}_n$. (Notice this is differenty defined from what was used in the weekly exercises)\n",
    "\n",
    "1. Consider a dataset composed of four points. Assume we compute a similarity matrix uses a threshold function on Euclidean (norm-2) distance. The metric outputs 1 if the points are close enough according to a threshold and zero otherwise. Consider two cases: \n",
    "    1. When there are two pairs of points close to each other (see image below, left)\n",
    "    2. When there is only one point isolated from the others (see image below, right)\n",
    "    <div>\n",
    "    <img src=\"images/spectral_1.png\" width=\"300\"><img src=\"images/spectral_2.png\" width=\"300\">\n",
    "    </div>\n",
    "    2. Build the Laplacian in each case and discuss the eigenvalues and eigenvectors.\n",
    "2. Consider the graph below that has no edge among nodes (see picture below). In such a graph, compute the modularity as a function of the number of nodes, of a clustering $S$ which assigns each cluster a single node. \n",
    "    \n",
    "    ![image.png](images/modularity.png)\n",
    "3. Given the modularity definition $\\displaystyle Q(G,\\mathcal{C})=\\frac{1}{2 m} \\sum_{c \\in \\mathcal{C}} \\sum_{i \\in c} \\sum_{j \\in c}\\left(a_{i j}-\\frac{d_{i} d_{j}}{2 m}\\right)$, what is the partition with _minimum_ modularity? Motivate your answer. \n",
    "4. Prove that the modularity of a partition having all nodes in one cluster is $\\frac{1}{4}$ irrespective of the number of nodes of the graph $G$.\n",
    "5.  Let $G$ be a graph with largest Laplacian eigenvalue $\\lambda_1$ and maximum degree $\\Delta > 0$. Show that $\\lambda_1 \\geq \\Delta + 1$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random walks and PageRank\n",
    " \n",
    "Recall that the PageRank is defined as $\\mathbf{r} = \\alpha \\mathbf{Mr} + (1-\\alpha)\\mathbf{p}$, where $\\mathbf{r}$ is the PageRank vector, $\\alpha$ is the restart probability, $\\mathbf{M} = A^\\top\\Delta^{-1}$, and $\\mathbf{p}$ is the restart (or personalization) vector. \n",
    "\n",
    "\n",
    "1. What is the PageRank of a complete bipartite graph with 3 nodes on one side and 4 on the other side and $\\alpha=1$?  \n",
    "2. You are given a complete subgraph (i.e., a clique) with a dangling node (number 1, in the image below)\n",
    "    1. Compute, __and show the computation steps__, the PageRank (i.e. $\\alpha=1$) for the graph below when the number of nodes $n=4$ and the clique has 3 nodes. **Note**: In this question you should compute PageRank by hand solving the system of equations or via Power Iteration\n",
    "    2. Show what happens to the PageRank when you increase by 1 node the size of the clique in the graph in point 1. What is the PageRank of the nodes __not__ connected with the isolated node? What is the PageRank of the isolated node? **Note**: No computation needed, only reasoning \n",
    "    3. Assume now the graph has $n$ nodes (where $n$ is a variable) and a clique with $n-1$ nodes. What is the PageRank of  the $n-2$ nodes in the clique that are not connected to the isolated node? Provide sufficient motivation for your argument or, if you can, a formula which depends on $n$.\n",
    "    4. Assume you have embedded the graph with a __Linear Embedding__ using unormalized Laplacian matrix of the graph as the similarity matrix. How do you expect the embeddings to be if the embedding dimension is $d = 1$? Motivate your answer with a mathematical reasoning.\n",
    "    \n",
    "    ![image.png](images/pagerank.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spectral clustering\n",
    "We will now proceed with community detection algorithms. \n",
    "For this part of the hand in, we will try to cluster a subset of the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, *_ = load_mnist()\n",
    "np.random.seed(1)  # DO NOT CHANGE\n",
    "\n",
    "n, h, w = X_train.shape\n",
    "sel = np.random.permutation(n)\n",
    "X = X_train[sel]\n",
    "y = y_train[sel]\n",
    "\n",
    "fig, ax = plt.subplots(2,6, figsize=(10, 3))\n",
    "for i in range(2): \n",
    "    for j in range(6):\n",
    "        ax[i, j].imshow(X[i*4 + j])\n",
    "        ax[i, j].set_title(\"Label: %d\" % y[i*4 + j])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load 200 random samples from the MNIST training set.\n",
    "X_ = X[:200].reshape(-1, 784)\n",
    "X_ = (X_ - X_.mean(1, keepdims=True)) / X_.std(1, keepdims=True)\n",
    "y_ = y[:200]\n",
    "\n",
    "def plot_neighborhood_graph(G, pos, ax):\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    nx.draw_networkx_edges(G,pos=pos,ax=ax, alpha=0.3)\n",
    "    nx.draw_networkx_nodes(G,pos=pos,ax=ax, node_color=y_, node_size=50, cmap=plt.get_cmap('tab10'))\n",
    "    \n",
    "    ax.set_xlim(-1.1,1.1)\n",
    "    ax.set_ylim(-1.1,1.1)\n",
    "    \n",
    "def plot_img_neighborhood_graph(G, X, pos, ax):\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    trans=ax.transData.transform\n",
    "    trans2=fig.transFigure.inverted().transform\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos=pos, ax=ax, alpha=0.3)\n",
    "    \n",
    "    ax.set_xlim(-1.1,1.1)\n",
    "    ax.set_ylim(-1.1,1.1)\n",
    "    \n",
    "    # Add images to graph\n",
    "    piesize=0.02 # this is the image size\n",
    "    p2=piesize/2.0\n",
    "    for n in G.nodes:\n",
    "        xx,yy=trans(pos[n]) # figure coordinates\n",
    "        xa,ya=trans2((xx,yy)) * np.array([1.001, 0.93]) + np.array([0., 0.04]) # axes coordinates\n",
    "        a = plt.axes([xa-p2,ya-p2, piesize, piesize])\n",
    "        # a = plt.axes([xa-p2,ya-p2, piesize, piesize])\n",
    "        a.set_aspect('equal')\n",
    "        a.imshow(X_[n].reshape(28, 28))\n",
    "        a.axis('off')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take the dataset and construct the $\\varepsilon$-neighborhood graph, using Eucledian (L2) distance. Try with different epsilons and plot the graphs. What do you observe as epsilon increases? You can use the code below to plot the graph if you wish. Fix the $\\varepsilon = 20$ in the rest of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are allowed to use sklearn if you wish. \n",
    "# Be sure that your constructed graphs does not contain loop edges (edges from i to i for some node i)\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "\n",
    "\n",
    "def nn_graph(X, eps=20):\n",
    "    G = None\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with eps-neighborhood graph for different values of eps. You can use code below for plotting\n",
    "\n",
    "# Use the reduced data-set X_. Otherwise computation and plotting will take much too long.\n",
    "G = nn_graph(X_)\n",
    "pos=nx.spring_layout(G)\n",
    "\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "ax=plt.subplot(121)\n",
    "plot_neighborhood_graph(G, pos, ax)\n",
    "\n",
    "ax=plt.subplot(122)\n",
    "plot_img_neighborhood_graph(G, X, pos, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assign to each edge in the $\\varepsilon$-neighborhood graph a weight\n",
    "    $$W_{i j}=e^{-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^{2}}{t}}$$\n",
    "   where $t\\geq 0$ is a parameter. What happens when $t$ is very small, close to $0$, i.e., $t \\rightarrow 0$? What happens when $t$ is very large? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_nn_graph(X, eps=20, t=0.01):\n",
    "    G = None\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare the eigenvectors of the weighted adjacency matrix, the weighted normalized laplacian, and the weighted random walk laplacian. Plot the eigenvectors of the different matrices. How do they relate? How do the matrix values vary with $t$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run spectral clustering with the weighted random walk and the weighted normalized laplacian with $t = 0.01$. You should implement spectral clustering yourself. You are allowed to use your implementation from the weekly exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Record the performance with Normalized Mutual Information (NMI) you implemented in the first Handin. What is the value of $t \\in \\{0.001,0.01,0.1,1,10,100,1000\\}$ and a number of communities $k \\in \\{2,3,5,10\\}$ that maximizes the NMI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. There seems to be a relationships between embeddings and spectral clustering, can you guess that? _Hint_: Think to the eigenvectors of the graph's Laplacians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Link analysis\n",
    "In this exercise, we will work with PageRank, Random Walks and their relationships with graph properties. \n",
    "We will use the most generic definition\n",
    "\n",
    "$$\\mathbf{r} = \\alpha \\mathbf{Mr} + (1-\\alpha)\\mathbf{p}$$\n",
    "\n",
    "with $\\mathbf{r}$ the PageRank vector, $\\mathbf{M}$ the weighted transition matrix, and $\\mathbf{p}$ the personalization vector. Additionally, let $n = |V|$, where $V$ is the nodes in the graph above (should be 200 nodes). \n",
    "\n",
    "\n",
    "1. Define and compute the transition (PageRank) matrix for the weighted graph in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the weighted graph constructed in the previous exercise with the values of $\\varepsilon=20$ and $k$ according to best NMI, run PageRank with $\\alpha = 0.85$, and vary $t \\in \\{0,0.001,0.01,0.1,1,10,100,1000\\}$ (test all values in the range). What do you notice as $t$ varies? With different values of $t$, what is the node with the highest PageRank? What are the 5 nodes with the lowest PageRank? How do they change with t? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the same graph, run PageRank with $t=0.01$ and $\\alpha \\in \\{0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1\\}$. Do you notice some relationship between $t$ and $\\alpha$ and PageRank? How do you explain that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now study the effect of spam in the network and construct a link farm. For simplicity treat all weights in the graph as binary. That is, $w_{ij} = 1$ if $j$ is in the $\\varepsilon$-neighborhood of $i$, i.e., $j \\in N_\\varepsilon(i)$, and $w_{ij}=0$ otherwise. \n",
    "\n",
    "1. Based on the analysis in the slides, construct a spam farm $s$ on the graph above with $T$ fake nodes. Assume that $s$ manages to get links from all the node that refers to the digit 1. With $\\alpha=0.85$, how many pages $T$ do you need to add in order to get $s$ being assigned the highest PageRank? Provide sufficient theoretical justification. To help your analysis you can initially provide a piece of code that finds $T$ such that the PageRank $\\mathbf{r}_s$ is maximizied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the above scenario, assume that $T = \\frac{1}{5}$ of the nodes in the original graph. What value of $\\alpha$ will maximize the PageRank $\\mathbf{r}_s$ of the link farm $s$? Provide sufficient justification for your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now we fix both $\\alpha = 0.85$ and $T = \\frac{1}{5}n$. Implement the method for spam mass estimation and check whether it is able to detect the node $s$, if the trusted set of nodes is a random sample $10\\%$ of the nodes in the original graph. If not, what could be a viable solution? Which nodes would you rather choose as trusted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph embeddings\n",
    "\n",
    "In this final exercise, we will try a different approach for clustering the data from above. Assume the graph is **not** weighted and use the graph with $\\varepsilon = 20$.\n",
    "The strategy is going to be the following:\n",
    "\n",
    "1. Use VERSE [[1]](https://arxiv.org/pdf/1803.04742.pdf) to produce embeddings of the nodes in the graph.\n",
    "2. Use K-Means to cluster the embeddings. Measure and report NMI for the clustering. \n",
    "3. Now assume the graph is weighted with weights in exercise 2. Tune the parameter $t$ to obtain the best NMI. What do you notice in terms of weights? In particular, is $t$ large or small and what does it imply for the structure? \n",
    "\n",
    "Your task is to i) fill in the methods below to implement the sampling version of VERSE. _Hint:_ it might be a help to look in the original article.\n",
    "and ii) run K-means on the embeddings to evaluate the performance compared to Spectral clustering using the NMI as measure.\n",
    "\n",
    "[[1](https://arxiv.org/pdf/1803.04742.pdf)] Tsitsulin, A., Mottin, D., Karras, P. and Müller, E., 2018, April. Verse: Versatile graph embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference (pp. 539-548)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ''' Return the sigmoid function of x \n",
    "        x: the input vector\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    ### YOUR CODE HERE\n",
    "    return x\n",
    "\n",
    "def pagerank_matrix(G, alpha = 0.85) :     \n",
    "    ''' Return the Personalized PageRank matrix of a graph\n",
    "\n",
    "        Args:\n",
    "            G: the input graph\n",
    "            alpha: the dumping factor of  PageRank\n",
    "\n",
    "        :return The nxn PageRank matrix P\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    ### YOUR CODE HERE\n",
    "    return P\n",
    "    \n",
    "\n",
    "def update(u, v, Z, C, step_size) :\n",
    "    '''Update the matrix Z using row-wise gradients of the loss function\n",
    "\n",
    "       Args:\n",
    "            u : the first node\n",
    "            v : the second node\n",
    "            Z : the embedding matrix\n",
    "            C : the classification variable used in Noise Contrastive estimation indicating whether the sample is positive or negative\n",
    "            step_size: step size for gradient descent\n",
    "\n",
    "\n",
    "       :return nothing, just update rows Z[v,:] and and Z[u,:]\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "def verse(G, S, d, k = 3, step_size = 0.0025, steps = 10000): \n",
    "    ''' Return the sampled version of VERSE\n",
    "\n",
    "        Args:\n",
    "            G: the input Graph\n",
    "            S: the PageRank similarity matrix\n",
    "            d: dimension of the embedding space\n",
    "            k: number of negative samples\n",
    "            step_size: step size for gradient descent\n",
    "            steps: number of iterations\n",
    "\n",
    "        :return the embedding matrix nxd\n",
    "    '''\n",
    "    n = G.number_of_nodes()\n",
    "    Z = 1/d*np.random.rand(n,d)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs the `verse` algorithm above on G and stores the embeddings to 'verse.npy'\n",
    "P   = pagerank_matrix(G)\n",
    "emb = verse(G, P, 128, step_size=0.0025, steps=10_000)\n",
    "np.save('verse.npy', emb)\n",
    "\n",
    "# TODO\n",
    "# Run K-means multiple times and choose the best.\n",
    "# Compare to Spectral clustering on G, according to NMI "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
